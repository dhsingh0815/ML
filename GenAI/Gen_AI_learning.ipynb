{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPH42yG9BPBlN8ylTGYJnmw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhsingh0815/ML/blob/main/GenAI/Gen_AI_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IrnejFctgpDQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# To run your app in WSL\n",
        "Install WSL extension in VScode\n",
        "Go to Terminal -> WSL Option\n",
        "It will show the linux shell the directory as\n",
        "#### dharmendrasingh@HOSTNAME:/mnt/c/Projects/GenAI$\n",
        "# Set up and activate virtual envionment\n",
        "Run command :\n",
        "python3 -m venv venv\n",
        "source venv/bin/activate\n",
        "Set-ExecutionPolicy -Scope Process -ExecutionPolicy Bypass\n",
        ".venv\\Scripts\\activate\n",
        "\n",
        "# run code\n",
        "I have project structure\n",
        "##### C:\\Projects\\GenAI\\genai\\main.py\n",
        "run  command : python genai/main.py\n",
        "\n",
        "# üì§ Output\n",
        "#### O/P with local inference model\n",
        "{'query': 'What is Attention?',  \n",
        " 'result': ' Attention is a mechanism used to focus on specific parts of an input sequence when processing that sequence. This allows the model to selectively attend to relevant parts of the input and ignore irrelevant parts, allowing the model to more effectively process and understand complex sequences of data.'}\n",
        "#### O/P with Groq hosted inference model\n",
        "{'query': 'What is Attention?', 'result': '**Attention** is a neural‚Äënetwork mechanism that lets a model ‚Äúlook back‚Äù at different parts of an input sequence (or of a previous layer‚Äôs output) and decide which parts are most relevant when it builds a new representation.  \\n\\n---\\n\\n### How it works (in a nutshell)\\n\\n1. **Three vectors per position**  \\n   For every token (or position) we create three vectors:  \\n   * **Query** ‚Äì what we are looking for.  \\n   * **Key** ‚Äì what each token can offer.  \\n   * **Value** ‚Äì the actual information to be passed forward.\\n\\n2. **Similarity scores**  \\n   For a given query we compute a dot‚Äëproduct (or other similarity) with every key in the sequence.  \\n   These scores are then scaled and passed through a softmax, turning them into a probability distribution that sums to 1.\\n\\n3. **Weighted sum**  \\n   The attention output for that query is the weighted sum of all the value vectors, where the weights are the softmax scores.  \\n   Thus, the output focuses more on the positions whose keys are most similar to the query.\\n\\n---\\n\\n### Why it matters\\n\\n| Feature | What it gives you | Typical use |\\n|---------|-------------------|-------------|\\n| **Self‚Äëattention** | Every position can attend to every other position in the same sequence | Capturing long‚Äërange dependencies in a single layer (e.g., Transformers) |\\n| **Encoder‚Äëdecoder attention** | Decoder positions can attend to encoder positions | Aligning source and target in sequence‚Äëto‚Äësequence tasks (machine translation, summarization) |\\n| **Multi‚Äëhead attention** | Parallel attention ‚Äúheads‚Äù learn different patterns or relations | Improves expressiveness without increasing model size |\\n\\n---\\n\\n### Practical impact\\n\\n* **Parallelism** ‚Äì Unlike recurrent networks, attention can process all positions simultaneously, speeding up training and inference.  \\n* **Long‚Äëdistance capture** ‚Äì A token can directly attend to any other token, no matter how far apart, which is especially useful for long documents.  \\n* **Interpretability** ‚Äì The attention weights can be visualized to see which parts of the input the model considered important for a given output.\\n\\n---\\n\\nIn short, **attention is a learned, weighted ‚Äúglimpse‚Äù over an input sequence that lets a model focus on the most relevant pieces of information when forming its internal representations.** It is a core component of modern architectures such as the Transformer, which rely on it for tasks ranging from translation to text summarization and beyond.'}\n",
        " # üìò Issues Faced & Learnings:\n",
        " #### Learnings\n",
        " ~ in Unix represents your home directory e.g /home/dharmendrasingh  \n",
        ".cache represnets a folder named .cache but it is a hidden folder because of .  \n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\") - This represents a Embedding model  \n",
        "\n",
        "llm = LlamaCpp(\n",
        "    model_path=\"~/models/mistral/mistral.gguf\",  # e.g., 'llama-2-7b-chat.gguf'\n",
        "    n_ctx=2048,\n",
        "    n_gpu_layers=20,  # if you have GPU acceleration\n",
        "    verbose=True\n",
        ")\n",
        "represents a LLM model  \n",
        "\n",
        "#### Saving Embeddings to Local so that on every run it is not computed\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "    model.save(\"/home/dharmendrasingh/models/huggingface\")\n",
        "#### Splitter\n",
        "There are different types of text splitters, these class split or create chunks from the original document.\n",
        "LLM has restrictions on number of tokens it can allow at a time,Initially it was default value but later used\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,       # characters (reduce if needed)\n",
        "    chunk_overlap=50\n",
        ")\n",
        "#### Using Local Inference Model or Hosted Inference Model\n",
        "Earlier I installed model locally and used\n",
        "llm = LlamaCpp(\n",
        "    model_path=\"/home/dharmendrasingh/models/mistral/mistral-7b-instruct-v0.1.Q2_K.gguf\",  # e.g., \"/home/dharmendrasingh/models/mistral/mistral.gguf\"\n",
        "    n_ctx=2048,\n",
        "    n_gpu_layers=20,  # if you have GPU acceleration\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "But It was very slow taking some 20 mins , then I switched to free tier of Groq hosted Chat models.\n",
        "llm = ChatOpenAI(\n",
        "    model=\"openai/gpt-oss-20b\",  # Choose from Groq-supported models\n",
        "    temperature=0.5\n",
        ")\n",
        "** You will need to define OPENAI_API_KEY & OPENAI_API_BASE (https://api.groq.com/openai/v1/) url.\n",
        "\n",
        "# ‚ùå Issues & Random thoughts\n",
        "## How to run ipynb files in VS code."
      ],
      "metadata": {
        "id": "cOnOMvetgun1"
      }
    }
  ]
}